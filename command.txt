gpu-l40s: 2 nodes. Per node: 2x32 core CPUs, 256GB of RAM, and 4 L40S GPUs. Max runtime for a job is 24 hours. 

sinfo -p gpu-l40s
scontrol show node g101
: check if available

: interactively run job
srun --x11 --partition=gpu-l40s --nodes=1 --ntasks=1 --cpus-per-task=16 --gres=gpu:1 --time=24:00:00 --pty bash
: "--pty bash" opens an interative bash terminal; could be "--pty python" for python REPL session
OR
salloc --x11 --partition=gpu-l40s --nodes=1 --ntasks=1 --cpus-per-task=16 --gres=gpu:1 --time=24:00:00 
: allocate a node
: -w g102 (tested good with o3d.draw_geometries)
: then "srun -pty bash" to start a shell; or "srun python train.py" to run a job; "srun --pty to keep the window interactive"

cd "/mmfs1/projects/smartlab/code/"
export PYTHONPATH=.
module load cuda12.4
source ~/myenv/bin/activate
: when run job from /code

nvidia-smi
: to test if gpu ready after loading cuda

: when run on multi-node multi-gpu
srun --x11 --partition=gpu-l40s --nodes=2 --ntasks-per-node=4 --cpus-per-task=16 --gres=gpu:4 --time=24:00:00 --pty bash
export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export MASTER_PORT=12355

squeue -u yfang24
squeue -j jid
: check job status

scancel jid
scancel -u yfang24
: after cancelling, slurm may be confused; use "srun --jobid=294170 --pty bash" to open a new shell

cat .out
tail -f .out

python3 -m venv ~/myenv
: to create an env

deactivate
: env

echo $VIRTUAL_ENV
: check current env

rm -rf slurm/*
: delete all files under slurm/

export OPEN3D_RENDERING_BACKEND=osmesa   # or egl

#===========
: non-interactively run job
srun --partition=gpu-l40s --nodes=1 --ntasks=1 --cpus-per-task=64 --gres=gpu:1 --time=01:00:00 python train.py
# srun is bound to terminal; jobs will be terminated if terminal is closed
# if want to do jobs in the background, use sbatch

or
1. create sample.sh:
#!/bin/bash 
#SBATCH --job-name=myjob   
#SBATCH --partition=gpu-l40s   
#SBATCH --nodes=1   
#SBATCH --ntasks=1   
#SBATCH --cpus-per-task=64   
#SBATCH --gres=gpu:1
#SBATCH --time=01:00:00
#SBATCH --output=slurm-%j.out   
#SBATCH --error=slurm-%j.err   
#SBATCH --mail-user=yfang24@stevens.edu   
#SBATCH --mail-type=BEGIN,END,FAIL # Get email notifications for job start, end, and failure  
 
module load cuda12.4
source ~/myenv/bin/activate
srun python train.py
# could also be "srun ./program.sh" if the .sh file starts with #!/bin/bash


2. submit the job
sed -i 's/\r$//' gpu.sh # convert file to UNIX format
sbatch gpu.sh


#===========
#git
#===========
set pat permission: content (read/write)
git remote set-url origin https://github.com/your-username/new-repo.git # set/reset origin
nano .gitignore # adding folders and files to ignore
.git/ingo/exclude # add .gitignore
git rm -r --cached .  # clear .gitignore tracking; do every time when the file is changed
git add . # add all local files to commit
git status # check what has been changed locally/remotely since last commit
git commit -m "comment"
git push --set-upstream origin main # first push
git push --force origin main # overwrite current repo
git push origin main
git pull origin main # pull changes on repo and merge with local changes
git pull --rebase # pull changes on repo and apply local changes
git stash # stash local changes
git pull
git stash clear
git stash pop # apply stashed changes